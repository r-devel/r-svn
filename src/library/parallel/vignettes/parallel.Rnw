%\VignetteEncoding{UTF-8}
% File src/library/parallel/vignettes/parallel.Rnw
% Part of the R package, https://www.R-project.org
% Copyright 2011–2026 R Core Team
% Distributed under GPL 2 or later

\documentclass[a4paper]{article}

\usepackage{Rd, parskip, amsmath, enumerate}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{color}
\definecolor{Blue}{rgb}{0,0,0.8}
\hypersetup{%
  colorlinks,%
  plainpages=true,%
  linkcolor=black,%
  citecolor=black,%
  urlcolor=Blue,%
  pdfstartview={XYZ null null 1},%
  pdfview={XYZ null null null},%
  pdfpagemode=UseNone,%
  pdfauthor={R Core Team},%
  pdftitle={Package `parallel'}%
}

%\VignetteIndexEntry{Package 'parallel'}
%\VignettePackage{parallel}

\newcommand{\I}[1]{#1}

\title{Package `parallel'}
\author{R Core Team}

\begin{document}
\maketitle




\section{Introduction}
The \pkg{parallel} package was first included in \R{} 2.14.0. It provides
parallelized replacements for many common functions, with integrated
handling of random‑number generation.

Note: Earlier packages such as \pkg{snow} and \pkg{multicore} were merged into base R with the 
introduction of the \pkg{parallel} package in R~2.14.0 (October 2011), which is now the recommended 
framework for parallel computation. Related vignettes such as \pkg{GridR} and \pkg{rgse} are no longer 
maintained: their functionality has either been absorbed into base R (e.g., the \pkg{grid} graphics system) 
or superseded by modern tools such as \pkg{Sweave}, \pkg{knitr}, and R Markdown. This vignette therefore 
focuses only on the current \pkg{parallel} package and its supported workflows.


Parallelism can occur at many levels. This package is principally
concerned with \emph{coarse‑grained parallelization}. At the lowest level,
modern CPUs can perform several basic operations simultaneously (e.g.,
integer and floating‑point arithmetic). External \abbr{BLAS} libraries
often use multiple threads for vector/matrix operations, and several
contributed \R{} packages use multiple threads at C level via
\abbr{OpenMP} or \I{pthreads}.

The \pkg{parallel} package handles larger chunks of computation. A typical
example is evaluating the same \R{} function on many different datasets,
such as bootstrap computations. Crucially, these tasks are independent
and do not need to communicate. Often they take approximately the same
time. The basic computational model is:

\begin{enumerate}[(a)]
\item Start $M$ worker processes and perform any required initialization.
\item Send necessary data for each task to the workers.
\item Split the task into $M$ roughly equal chunks and send them (including
      the \R{} code) to the workers.
\item Wait for all workers to complete and collect their results.
\item Repeat steps (b–d) for further tasks.
\item Shut down the worker processes.
\end{enumerate}

Initialization may include loading packages and setting up random‑number streams.

Functions such as \code{mclapply} and \code{parLapply} implement this model
as near‑drop‑in replacements for \code{lapply}.

A variation is to split the task into $M_1 > M$ chunks, send the first $M$
chunks to the workers, then repeatedly assign new tasks to workers as they
finish — see the section on load balancing.

In practice, workers are implemented as full processes. They can be created
in two main ways:

\begin{enumerate}
\item \emph{Via} \code{system("Rscript")} or similar to launch a new process
      on the current machine (or a similar one with an identical \R{}
      installation). Communication between master and workers is usually
      via sockets. This should work on all \R{} platforms, although strict
      security settings may block socket communication. On Windows and
      \abbr{macOS}, firewall dialogs may appear asking if \R{} should accept
      incoming connections. A pool of worker processes listening via sockets
      is called a \emph{cluster} of nodes.

\item \emph{Via} forking. Forking is a POSIX concept\footnote{\url{https://en.wikipedia.org/wiki/Fork_(operating_system)}}
      available on all \R{} platforms except Windows. It creates a new
      \R{} process by copying the master process, including workspace and
      random‑number state. Memory pages are shared until modified, so
      forking is fast. Because the child shares the complete process, it
      also shares GUI elements (e.g., console, devices), which can cause
      issues.\footnote{On \abbr{macOS}, precautions are taken: event loops
      for \command{R.app} and the \code{quartz} device are inhibited in the
      child. At C level, this is indicated by the \code{Rboolean} variable
      \code{R\_isForkedChild}.} Communication between master and worker can
      use shared memory or sockets. In \pkg{parallel}, both socket‑based
      clusters and forked processes are supported.
\end{enumerate}

Parallel computing has evolved with the advent of shared‑memory computers
with multiple CPU cores. Today, laptops typically have two or four cores,
and servers with 8, 32, or more cores are common. \pkg{parallel} is designed
to exploit such hardware. It can also be used across multiple computers
running the same version of \R{} connected by ethernet, even if they use
different operating systems.

All communication methods use \code{serialize}/\code{unserialize} to send
\R{} objects between processes. This has limits (hundreds of millions of
elements), which well‑designed parallel algorithms should avoid.

\section{Numbers of CPUs/cores}
In setting up parallel computations it can be helpful to know the number
of CPUs or cores available, but this is a slippery concept. Almost all
physical CPUs now contain two or more cores that run more‑or‑less
independently (sharing cache memory and RAM access). On some processors,
these cores may themselves run multiple tasks simultaneously, and some
operating systems (e.g., Windows) expose the concept of \emph{logical}
CPUs, which can exceed the number of physical cores.

Note that programs can only determine the total number of CPUs/cores
available. This is not necessarily the number available \emph{to the
current user}, which may be restricted by system policies on multi‑user
systems. Nor does it indicate a reasonable number of CPUs to use for a
task: users may be running many \R{} processes simultaneously, and those
processes may themselves use multiple threads via multi‑threaded
\abbr{BLAS}, compiled code using \abbr{OpenMP}, or other low‑level
parallelism. There have even been instances of \code{mclapply} being called
recursively,\footnote{\code{parallel::mclapply} detects this and runs nested
calls serially.} generating $2n + n^2$ processes on a machine estimated to
have $n = 16$ cores.

As a guideline, the function \code{detectCores()} attempts to determine
the number of CPU cores on the machine running \R{}. It has methods for
all known current \R{} platforms. What it measures is OS‑specific: where
possible, the number of logical cores is reported.

On Windows, the default is to report logical CPUs. On modern hardware
(e.g., Intel \emph{Core i7}), this is reasonable since hyper‑threading
does provide significant throughput. What
\code{detectCores(logical = FALSE)} reports depends on the OS version:
recent versions of Windows report physical cores, while older versions
may report physical CPU packages.

\section{Analogues of apply functions}
The most common direct applications of \pkg{parallel} have been to
provide parallelized replacements of \code{lapply}, \code{sapply},
\code{apply}, and related functions.

As analogues of \code{lapply} there are:
\begin{verbatim}
parLapply(cl, x, FUN, ...)
mclapply(X, FUN, ..., mc.cores)
\end{verbatim}

Note that \code{mclapply} is not available\footnote{Except as a stub
which simply calls \code{lapply}.} on Windows. It has additional
arguments described in its help page. The two functions differ slightly
in philosophy: \code{mclapply} sets up a pool of \code{mc.cores} workers
just for the computation, whereas \code{parLapply} uses a persistent pool
specified by the object \code{cl} created by \code{makeCluster} (which
\emph{inter alia} specifies the pool size). A typical workflow is:

\begin{verbatim}
cl <- makeCluster(<size of pool>)
# one or more parLapply calls
stopCluster(cl)
\end{verbatim}



\section{Clusters}
The \pkg{parallel} package provides functions to create and manage
clusters of worker processes. Two main functions are available:

\begin{itemize}
  \item \code{makePSOCKcluster}: launches worker processes using
        \code{Rscript} on the current host or optionally on remote hosts.
  \item \code{makeForkCluster}: available on POSIX systems (not Windows),
        creates worker processes by forking the current session.
\end{itemize}

These functions are normally called via \code{makeCluster}, which
provides a unified interface. The difference lies in how workers are
spawned: \code{makePSOCKcluster} starts fresh \R{} sessions, while
\code{makeForkCluster} forks the current process, inheriting its
environment.

By default, worker output streams (\code{stdout()} and \code{stderr()})
are redirected. They are discarded unless the \code{outfile} option is
set, in which case logs can be captured. Note that this refers to the
\R{} connections, not low‑level C file handles. Properly written \R{}
packages using \code{Rprintf} will have their output redirected.

A default cluster can be registered with \code{setDefaultCluster()}.
This cluster is then used whenever higher‑level functions such as
\code{parApply} are called without an explicit cluster. Care is needed
when re‑using worker pools, as their workspaces accumulate objects and
packages from past usage.

When creating clusters on remote hosts, \code{makeCluster} may require
additional arguments:
\begin{itemize}
  \item For heterogeneous systems, use \code{homogeneous = FALSE} and
        specify the full path to \command{Rscript} on the workers.
  \item The master hostname is usually detected via \code{Sys.info()},
        but on private networks it may need to be set explicitly, e.g.
        \code{master = "192.168.1.111"}.
  \item By default, \command{ssh} is used to launch R on workers. If a
        different command is needed, specify it with \code{rshcmd}.
        SSH should be configured for silent authentication.
  \item Socket communication uses a random port in the range
        \code{11000:11999}. To override, set the \code{port} argument or
        environment variable \env{R\_PARALLEL\_PORT}.
\end{itemize}

\section{Forking}
On POSIX systems (not Windows), \pkg{parallel} supports forking. This
creates worker processes by duplicating the master process. Functions
such as \code{mclapply}, \code{mcmapply}, \code{mcMap}, and \code{pvec}
provide high‑level interfaces for parallel computation using forks.

By default, these functions use two cores, but the number can be
controlled via \code{options("mc.cores")} or the environment variable
\code{MC\_CORES}. Setting this to \code{1} disables parallelism. On
Windows, stub versions are provided that force \code{mc.cores = 1}.

Forked workers share the temporary directory \code{tempdir()}, which
can cause conflicts if code assumes it is private to each process.
They also share file handles with the master, meaning output from
workers is directed to the master’s \file{stdout} and \file{stderr}.
Setting \code{mc.silent = TRUE} suppresses \file{stdout} for child
processes. Graphics devices are also inherited, so forked workers
should not attempt to use them.

\section{Random‑number generation}
Parallel computation with random numbers requires care to ensure
independent and reproducible streams. One safe approach is to perform
all randomization in the master process and distribute pre‑computed
values to workers. Alternatively, separate seeds can be set for each
worker in a reproducible way.

\pkg{parallel} implements the ideas of \citet{lecuyer.2002}, providing
streams of random numbers separated by $2^{127}$ steps in a generator
with period about $2^{191}$. This is based on the combined multiple
recursive generator of \citet{lecuyer.1999}. Unlike the default
\code{"Mersenne‑Twister"} RNG, this generator allows efficient
advancement of the seed by a fixed number of steps.

The generator is available via \code{RNGkind("L'Ecuyer-CMRG")}. Streams
can be advanced using \code{nextRNGStream()}. For example:

<<Ecuyer-ex, eval=FALSE>>=
RNGkind("L'Ecuyer-CMRG")
set.seed(2002) # initialize
M <- 16 ## start M workers
s <- .Random.seed
for (i in 1:M) {
    s <- nextRNGStream(s)
    # send s to worker i as .Random.seed
}
@

\section{Random‑number generation}
Parallel computation with random numbers requires care to ensure
independent and reproducible streams. Worker processes must run
independent random‑number streams to avoid correlation.

One safe approach is to perform all randomization in the master process
and distribute pre‑computed values to workers. Alternatively, separate
seeds can be set for each worker in a reproducible way from the master
seed. This ensures reproducibility while avoiding overlap between
streams.

\pkg{parallel} implements the ideas of \citet{lecuyer.2002}, providing
streams of random numbers separated by $2^{127}$ steps in a generator
with period about $2^{191}$. This is based on the combined multiple
recursive generator of \citet{lecuyer.1999}. Unlike the default
\code{"Mersenne‑Twister"} RNG, this generator allows efficient
advancement of the seed by a fixed number of steps.

Apart from \emph{streams} ($2^{127}$ steps apart), there are
\emph{sub‑streams} starting from seeds $2^{76}$ steps apart. Function
\code{nextRNGSubStream} advances to the next substream.

The generator is available via \code{RNGkind("L'Ecuyer-CMRG")}. Streams
can be advanced using \code{nextRNGStream()}. For example:

<<Ecuyer-ex, eval=FALSE>>=
RNGkind("L'Ecuyer-CMRG")
set.seed(2002) # initialize
M <- 16 ## start M workers
s <- .Random.seed
for (i in 1:M) {
    s <- nextRNGStream(s)
    # send s to worker i as .Random.seed
}
@

\section{Load balancing}
An alternative strategy is to dynamically allocate tasks to workers,
known as \emph{load balancing}. This is implemented in
\code{mclapply(mc.preschedule = FALSE)}, \code{clusterApplyLB}, and
related wrappers.

Load balancing is useful when tasks vary significantly in computation
time or when nodes differ in capability. However, some caveats apply:

\begin{enumerate}[(a)]
\item Random number streams are allocated to nodes, so if tasks involve
      random numbers they may be non‑repeatable (allocation depends on
      workload). To ensure reproducibility, streams should be allocated
      per task rather than per node.
\item Communication overhead can be high if tasks are sent one at a
      time. For example, allocating 1000 tasks to 10 nodes individually
      may be inefficient. Load balancing is most effective when the
      number of tasks is moderately larger than the number of nodes.
\end{enumerate}

\section{Setting CPU affinity with \code{mclapply}}
The parameter \code{affinity.list} of \code{mclapply} can be used to
assign elements of the input vector \code{X} to specific CPUs. It is a
vector (atomic or list) containing CPU affinity masks for each element
of \code{X}, describing which CPU (core or hyper‑thread unit) a given
item may run on (see \code{? mcaffinity}).

This is helpful when jobs vary in completion time or when hardware is
heterogeneous. It also enables scheduling strategies to optimize overall
runtime. If \code{affinity.list} is set, the \code{mc.cores} parameter
is replaced with the number of CPU IDs in the affinity masks. To use
this parameter, prescheduling must be deactivated
(\code{mc.preschedule = FALSE}).

For each value of \code{X}, a separate job is forked. The master process
forks one child per selected CPU at a time, ensuring the number of jobs
does not exceed the number of CPUs. As soon as a child finishes, the
next job is forked.

The following example demonstrates how execution time can be reduced by
assigning elements of \code{X} to specific CPUs (not available on
Windows, where \code{mc.cores} must remain at 1):

<<affinity-ex, eval=FALSE>>=
library(parallel)
n <- 300   # observations
p <- 20000 # covariates

library(stats)
A <- matrix(replicate(p, rnorm(n, sd = runif(1, 0.1, 10))), n, p)
B <- matrix(replicate(p, rnorm(n, sd = runif(1, 0.1, 10))), n, p)
C <- matrix(replicate(2*p, rnorm(n, sd = runif(1, 0.1, 10))), n, 2*p)

varFilter <- function (X, nSim = 20) {
  for (i in 1:nSim) {
    train <- sample(nrow(X), 2 / 3 * nrow(X))
    colVars <- apply(X[train, ], 2, var)
    keep <- names(head(sort(colVars, decreasing = TRUE), 100))
    # myAlgorithm(X[, keep])
  }
}



\section{Runtime comparison}
The following examples compare different strategies for parallel
execution using \code{mclapply}.

\subsection*{Using \code{affinity.list}}
CPU mapping: matrices A and B run on CPU 1, while matrix C runs on CPU 2.

<<eval=FALSE>>=
affinity <- c(1,1,2)
system.time(
  mclapply(X = list(A,B,C), FUN = varFilter,
           mc.preschedule = FALSE, affinity.list = affinity))
##   user  system elapsed
## 34.909   0.873  36.720
@

\subsection*{Without \code{affinity.list}}
<<eval=FALSE>>=
system.time(
  mclapply(X = list(A,B,C), FUN = varFilter, mc.cores = 2,
           mc.preschedule = FALSE))
##   user  system elapsed
## 72.893   1.588  55.982
@

\subsection*{With prescheduling}
<<eval=FALSE>>=
system.time(
  mclapply(X = list(A,B,C), FUN = varFilter, mc.cores = 2,
           mc.preschedule = TRUE))
##   user  system elapsed
## 53.455   1.326  53.399
@

\section{Restricting computation to specific CPUs}
Instead of using \code{affinity.list} for runtime optimization, it can
also be used to restrict computation to specific CPUs. For example:

<<eval=FALSE>>=
## Restrict all elements of X to run on CPU 1 and 2.
X <- list(1, 2, 3)
affinity.list <- list(c(1,2), c(1,2), c(1,2))
mclapply(X = X, FUN = function(i) i*i,
         mc.preschedule = FALSE, affinity.list = affinity.list)
@

\section{Portability considerations}
Developers providing parallel facilities in their code need to balance
portability and efficiency: no single approach works optimally on all
platforms.

Using \code{mclapply} is usually the simplest approach, but it runs in
serial mode on Windows. This may suffice when parallel computation is
only required on a single multi‑core Unix‑like system, since
\code{mclapply} can only run on shared‑memory systems. There is a
fallback to serial use by setting \code{mc.cores = 1}.

Using \code{parLapply} works wherever socket communication is available.
It can be used, for example, to harness all CPU cores on a single
machine or across multiple machines in a lab. However, socket
communication may be blocked by system policies, even on a single
machine, and is often restricted between machines. There is no automatic
fallback to serial use, since workers start with a different \R{}
environment than the master.

A practical example of providing access to both approaches, as well as
serial execution, is found in package \CRANpkg{boot} (version \code{1.3-3}
or later).

\section{Extended examples}
\SweaveOpts{eval=FALSE}
<<hide=TRUE>>=
library(parallel)
@

One of the most common uses of coarse‑grained parallelization in
statistics is to perform multiple simulation runs, such as large numbers
of bootstrap replicates or repeated runs of an MCMC simulation. We show
an example of bootstrapping.

Note: some examples will only run serially on Windows, and some are
computationally intensive.

\subsection{Bootstrapping}
Package \CRANpkg{boot} \citep{boot} supports the monograph by
\citet{Davison.Hinkley.97}. Bootstrapping is often used as an example of
easy parallelization, and some confidence interval methods require many
thousands of bootstrap samples. Since version \code{1.3-1}, the package
itself has parallel support within its main functions, but here we
illustrate how to use the original (serial) functions in parallel
computations.

We consider two examples using the \code{cd4} dataset from package
\CRANpkg{boot}, where the interest is in the correlation between before
and after measurements. The first is a parametric bootstrap simulation.
The non‑parallel form is:

<<>>=
library(boot)
cd4.rg <- function(data, mle) MASS::mvrnorm(nrow(data), mle$m, mle$v)
cd4.mle <- list(m = colMeans(cd4), v = var(cd4))
cd4.boot <- boot(cd4, corr, R = 999, sim = "parametric",
                 ran.gen = cd4.rg, mle = cd4.mle)
boot.ci(cd4.boot, type = c("norm", "basic", "perc"),
        conf = 0.9, h = atanh, hinv = tanh)
@

To parallelize this with \code{mclapply}, we break it into separate runs.
Here we illustrate two runs of 500 simulations each:

<<>>=
cd4.rg <- function(data, mle) MASS::mvrnorm(nrow(data), mle$m, mle$v)
cd4.mle <- list(m = colMeans(cd4), v = var(cd4))
run1 <- function(...) boot(cd4, corr, R = 500, sim = "parametric",
                           ran.gen = cd4.rg, mle = cd4.mle)
mc <- 2 # set as appropriate for your hardware
## To make this reproducible:
set.seed(123, "L'Ecuyer")
cd4.boot <- do.call(c, mclapply(seq_len(mc), run1))
boot.ci(cd4.boot, type = c("norm", "basic", "perc"),
        conf = 0.9, h = atanh, hinv = tanh)
@

Encapsulating the computation in a function is often the neatest
approach. The parallel form above corresponds to the serial form:

<<eval=FALSE>>=
do.call(c, lapply(seq_len(mc), run1))
@


\subsection{Bootstrapping with \code{parLapply}}
To run the bootstrap example with \code{parLapply}, we can take a similar
approach:

<<>>=
run1 <- function(...) {
   library(boot)
   cd4.rg <- function(data, mle) MASS::mvrnorm(nrow(data), mle$m, mle$v)
   cd4.mle <- list(m = colMeans(cd4), v = var(cd4))
   boot(cd4, corr, R = 500, sim = "parametric",
        ran.gen = cd4.rg, mle = cd4.mle)
}
cl <- makeCluster(mc)
## make this reproducible
clusterSetRNGStream(cl, 123)
library(boot) # needed for c() method on master
cd4.boot <- do.call(c, parLapply(cl, seq_len(mc), run1))
boot.ci(cd4.boot, type = c("norm", "basic", "perc"),
        conf = 0.9, h = atanh, hinv = tanh)
stopCluster(cl)
@

With \code{mclapply}, packages and objects are automatically available
to workers. With \code{parLapply}, this is not generally the case
(except with clusters created by \code{makeForkCluster}). It is often
necessary to decide where to perform computations: either on the workers
or on the master, exporting values as needed. For example:

<<>>=
cl <- makeCluster(mc)
cd4.rg <- function(data, mle) MASS::mvrnorm(nrow(data), mle$m, mle$v)
cd4.mle <- list(m = colMeans(cd4), v = var(cd4))
clusterExport(cl, c("cd4.rg", "cd4.mle"))
junk <- clusterEvalQ(cl, library(boot)) # discard result
clusterSetRNGStream(cl, 123)
res <- clusterEvalQ(cl, boot(cd4, corr, R = 500,
                    sim = "parametric", ran.gen = cd4.rg, mle = cd4.mle))
library(boot) # needed for c() method on master
cd4.boot <- do.call(c, res)
boot.ci(cd4.boot, type = c("norm", "basic", "perc"),
        conf = 0.9, h = atanh, hinv = tanh)
stopCluster(cl)
@

\subsection{Double bootstrap}
Running a double bootstrap on the same problem is far more
computationally intensive. The standard version is:

<<fig=TRUE>>=
R <- 999; M <- 999 ## we would like at least 999 each
cd4.nest <- boot(cd4, nested.corr, R=R, stype="w", t0=corr(cd4), M=M)
## nested.corr is a function in package boot
op <- par(pty = "s", xaxs = "i", yaxs = "i")
qqplot((1:R)/(R+1), cd4.nest$t[, 2], pch = ".", asp = 1,
        xlab = "nominal", ylab = "estimated")
abline(a = 0, b = 1, col = "grey")
abline(h = 0.05, col = "grey")
abline(h = 0.95, col = "grey")
par(op)

nominal <- (1:R)/(R+1)
actual <- cd4.nest$t[, 2]
100*nominal[c(sum(actual <= 0.05), sum(actual < 0.95))]
@

This took about 55 seconds on one core of an 8‑core Linux server.

Using \code{mclapply}, we can parallelize the computation:

<<eval=FALSE>>=
mc <- 9
R <- 999; M <- 999; RR <- floor(R/mc)
run2 <- function(...)
    boot(cd4, nested.corr, R=RR, stype="w", t0=corr(cd4), M=M)
cd4.nest <- do.call(c, mclapply(seq_len(mc), run2, mc.cores = mc))
nominal <- (1:R)/(R+1)
actual <- cd4.nest$t[, 2]
100*nominal[c(sum(actual <= 0.05), sum(actual < 0.95))]
@

This ran in about 11 seconds (elapsed) using all cores on that server.

\subsection{MCMC runs}
\citet{Ripley.88} discusses maximum‑likelihood estimation of the Strauss
process, solved by the moment equation


\[
E_c T = t
\]


where $T$ is the number of $R$‑close pairs and $t$ is the observed
value, $30$ in the following example. A serial approach might be:

<<>>=
library(spatial)
towns <- ppinit("towns.dat")
tget <- function(x, r=3.5) sum(dist(cbind(x$x, x$y)) < r)
t0 <- tget(towns)
R <- 1000
c <- seq(0, 1, 0.1)
res <- c(0, sapply(c[-1], function(c)
    mean(replicate(R, tget(Strauss(69, c=c, r=3.5))))))
plot(c, res, type="l", ylab="E t")
abline(h=t0, col="grey")
@

This takes about 20 seconds today, but many hours when first done in
1985. A parallel version might be:

<<>>=
run3 <- function(c) {
    library(spatial)
    towns <- ppinit("towns.dat") # has side effects
    mean(replicate(R, tget(Strauss(69, c=c, r=3.5))))
}
cl <- makeCluster(10, methods = FALSE)
clusterExport(cl, c("R", "towns", "tget"))
res <- c(0, parSapply(cl, c[-1], run3)) # 10 tasks
stopCluster(cl)
@

This took about 4.5 seconds, plus 2 seconds to set up the cluster. Using
a fork cluster (not on Windows) makes startup faster and setup easier:

<<eval=FALSE>>=
cl <- makeForkCluster(10)  # fork after variables have been set up
run4 <- function(c) mean(replicate(R, tget(Strauss(69, c=c, r=3.5))))
res <- c(0, parSapply(cl, c[-1], run4))
stopCluster(cl)
@

The \code{mclapply} version is slightly simpler:

<<eval=FALSE>>=
run4 <- function(c) mean(replicate(R, tget(Strauss(69, c=c, r=3.5))))
res <- c(0, unlist(mclapply(c[-1], run4, mc.cores = 10)))
@

If fewer than 10 cores are available, load balancing may be useful since
simulation time varies with \code{c}. This can be done using
\code{mclapply(mc.preschedule = FALSE)} or \code{parSapplyLB}. The
disadvantage is that results will not be reproducible, which may not
matter in exploratory runs.

\subsection{Package installation}
With thousands of \R{} packages available, it is often helpful to perform
a comprehensive install in parallel. Facilities exist in
\code{install.packages} via a parallel \command{make}, but that approach
may not always be suitable.\footnote{A parallel \command{make} might not
be available, and in some cases package installation has failed when run
from \command{make}.} Some tasks take much longer than others, and this
cannot be predicted in advance.

We illustrate an approach using \pkg{parallel}, as used on part of the
CRAN check farm. Suppose there is a function \code{do_one(pkg)} which
installs a single package and then returns. The task is to run
\code{do_one} on as many of the \code{M} workers as possible, while
ensuring that all direct and indirect dependencies of \code{pkg} are
installed before \code{pkg} itself. Since installing one package can
block several others, the number of installs running simultaneously must
be allowed to vary. The following code achieves this using low‑level
functions:

<<eval=FALSE>>=
pkgs <- "<names of packages to be installed>"
M <- 20 # number of parallel installs
M <- min(M, length(pkgs))
library(parallel)
unlink("install_log")
cl <- makeCluster(M, outfile = "install_log")
clusterExport(cl, c("tars", "fakes", "gcc")) # variables needed by do_one

## set up available via a call to available.packages() for
## repositories containing all the packages involved and all their
## dependencies.
DL <- utils:::.make_dependency_list(pkgs, available, recursive = TRUE)
DL <- lapply(DL, function(x) x[x %in% pkgs])
lens <- sapply(DL, length)
ready <- names(DL[lens == 0L])
done <- character() # packages already installed
n <- length(ready)
submit <- function(node, pkg)
    parallel:::sendCall(cl[[node]], do_one, list(pkg), tag = pkg)
for (i in 1:min(n, M)) submit(i, ready[i])
DL <- DL[!names(DL) %in% ready[1:min(n, M)]]
av <- if(n < M) (n+1L):M else integer() # available workers
while(length(done) < length(pkgs)) {
    d <- parallel:::recvOneResult(cl)
    av <- c(av, d$node)
    done <- c(done, d$tag)
    OK <- unlist(lapply(DL, function(x) all(x %in% done)))
    if (!any(OK)) next
    p <- names(DL)[OK]
    m <- min(length(p), length(av)) # >= 1
    for (i in 1:m) submit(av[i], p[i])
    av <- av[-(1:m)]
    DL <- DL[!names(DL) %in% p[1:m]]
}
@

\subsection{Passing \code{...}}
The semantics of \code{...} do not fit well with parallel operation,
since lazy evaluation may be delayed until tasks are sent to workers.
This is not a problem in the forking approach, as the information needed
for lazy evaluation is present in the forked workers.

For socket clusters, the trick is to ensure that \code{...} has any
promises forced while the information is still available. For example,
package \CRANpkg{boot} evaluates promises explicitly before passing them
to workers:

<<eval=FALSE>>=
fn <- function(r) statistic(data, i[r, ], ...)
RR <- sum(R)
res <- if (ncpus > 1L) {
    if (.Platform$OS.type != "windows") {
        parallel::mclapply(seq_len(RR), fn, mc.cores = ncpus)
    } else {
        list(...) # evaluate any promises
        if (is.null(cl)) {
            cl <- parallel::makePSOCKcluster(rep("localhost", ncpus))
            if (RNGkind()[1L] == "L'Ecuyer-CMRG")
                parallel::clusterSetRNGStream(cl)
            res <- parallel::parLapply(cl, seq_len(RR), fn)
            parallel::stopCluster(cl)
            res
        } else parallel::parLapply(cl, seq_len(RR), fn)
    }
} else lapply(seq_len(RR), fn)
@

Note that \code{...} is an argument to \code{boot}, and so forcing
evaluation with \code{list(...)} ensures promises are resolved before
parallel execution.
@ 
\section{Differences from earlier versions}
The \pkg{parallel} package has evolved over time, consolidating and
streamlining functionality for parallel computation in \R{}. Earlier
packages such as \CRANpkg{multicore} and \CRANpkg{snow} are now obsolete,
and their functionality has been integrated or replaced.

\subsection{Current functionality}
\begin{itemize}
  \item Parallel random‑number generation is fully supported, using
        reproducible streams via \code{RNGkind("L'Ecuyer-CMRG")} and
        functions such as \code{nextRNGStream()} and
        \code{nextRNGSubStream()}.
  \item Functions for managing worker processes are standardized:
        \code{makeCluster}, \code{makePSOCKcluster}, and
        \code{makeForkCluster} provide consistent interfaces across
        platforms.
  \item Worker logging is streamlined: output can be redirected or
        appended to log files for monitoring.
  \item High‑level functions such as \code{parLapply}, \code{parSapply},
        and \code{clusterMap()} are aligned with their serial
        counterparts (\code{lapply}, \code{sapply}, \code{mapply},
        \code{Map}), including support for arguments like
        \code{SIMPLIFY} and \code{USE.NAMES}.
  \item Load balancing is supported via \code{mclapply(mc.preschedule =
        FALSE)} and \code{parSapplyLB}, useful when tasks vary in
        runtime.
\end{itemize}

The package no longer provides legacy aliases or interfaces from older
parallel packages. Instead, it offers a unified, portable, and
maintained framework for parallel computation in \R{}.

\bibliographystyle{jss}
\bibliography{parallel}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
