% File src/library/parallel/vignettes/parallel.Rnw
% Part of the R package, https://www.R-project.org
% Copyright 2011-2017  R Core Team
% Distributed under GPL 2 or later

\documentclass[a4paper]{article}

\usepackage{Rd, parskip, amsmath, enumerate}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{color}
\definecolor{Blue}{rgb}{0,0,0.8}
\hypersetup{%
colorlinks,%
plainpages=true,%
linkcolor=black,%
citecolor=black,%
urlcolor=Blue,%
pdfstartview={XYZ null null 1},% 100%
pdfview={XYZ null null null},%
pdfpagemode=UseNone,% for no outline
pdfauthor={R Core Team},%
pdftitle={Package `parallel'}% Could also have pdfusetitle
}

%\VignetteIndexEntry{Package 'parallel'}
%\VignettePackage{parallel}

\newcommand{\I}[1]{#1}

\title{Package `parallel'}
\author{R Core Team}


\begin{document}

\maketitle

\section{Introduction}

The \pkg{parallel} package was incorporated into base \R{} starting with version 2.14.0. Its design synthesizes functionality from two prominent CRAN packages, \CRANpkg{multicore} \citep{multicore} and \CRANpkg{snow} \citep{snow}, offering largely compatible replacements for their core features while integrating improved management of random-number generation.

Parallel computation can be implemented at multiple architectural levels. This package focuses primarily on \emph{coarse-grained parallelization}. At the lowest hardware level, modern CPUs perform simultaneous basic operations (e.g., integer and floating-point arithmetic), and several external \abbr{BLAS} library implementations utilize multiple threads to parallelize sections of vector and matrix operations. Furthermore, several contributed \R{} packages employ multi-threading at the C level using \abbr{OpenMP} or \I{pthreads}.

In contrast, this package is concerned with executing substantially larger computational units in parallel. A typical use case involves evaluating the same \R{} function across numerous distinct datasets, such as in bootstrap simulations where each dataset comprises a different random sample. The essential characteristic of these tasks is their independence; they require no inter-process communication. It is often advantageous if the tasks require approximately equal computation time. The fundamental computational model employed is as follows:
\begin{enumerate}[(a)]
\item Initiate $M$ distinct \emph{worker} processes and perform any necessary initialization on each worker.
\item Transmit any data required for the tasks to the worker processes.
\item Partition the overall task into $M$ chunks of roughly equal size, then dispatch these chunks—including the requisite \R{} code—to the workers.
\item Await completion of all assigned tasks by the workers, then retrieve their results.
\item Repeat steps (b) through (d) for any subsequent tasks.
\item Terminate the worker processes.
\end{enumerate}
Common initialization steps include loading required packages and setting up independent random-number streams.

This model is implemented by functions such as \code{mclapply} and \code{parLapply}, which serve as near-direct substitutes for the standard \code{lapply} function.

A variant of this model partitions the task into $M_1 > M$ chunks. It sends the first $M$ chunks to the workers, then iteratively waits for any worker to finish and assigns it the next remaining chunk. This approach, known as \emph{load balancing}, is discussed in a later section.

In principle, workers could be implemented as threads\footnote{This remains a theoretical possibility because the \R{} interpreter itself is not thread-safe.} or lightweight processes, but the current implementation employs full operating-system processes. Workers can be created through one of three principal mechanisms:
\begin{enumerate}
\item Using \code{system("Rscript")} or a similar command to launch a new \R{} process on the local machine or on a remote machine with an identical \R{} installation. This necessitates a communication channel between the master and worker processes, typically established via network sockets.

  This method should be available on all platforms supporting \R{}, although stringent security configurations might block the required socket communication. Users of Windows and \abbr{macOS} may encounter firewall dialog boxes requesting permission for \R{} processes to accept incoming connections.

  Following the convention established by \CRANpkg{snow}, a collection of worker processes listening for commands from a master via sockets is termed a \emph{cluster} of nodes.

\item Using \emph{forking}. The \emph{fork} operation\footnote{\url{https://en.wikipedia.org/wiki/Fork_(operating_system)}} is a standard feature of POSIX-compliant operating systems and is available on all \R{} platforms except Windows. Forking creates a new \R{} process by duplicating the entire master process, including its workspace and the current state of the random-number generator. However, in any reasonable operating system, the child process initially shares memory pages with the parent until modifications occur, making forking an efficient operation.

  The use of forking for parallelism in \R{} was pioneered by the \CRANpkg{multicore} package.

  A significant consideration is that the forked child shares all aspects of the parent process, which includes any active graphical user interface (GUI) elements, such as an \R{} console or on-screen graphics devices. This can lead to problematic interactions.\footnote{Certain precautions are implemented on \abbr{macOS}; for example, the event loops for \command{R.app} and the \code{quartz} graphics device are disabled in the child process. This status is accessible at the C level via the \code{Rboolean} variable \code{R\_isForkedChild}.}

  Communication between the master and forked workers is required. Several methods are possible due to the shared memory space. The original \CRANpkg{multicore} implementation sent an \R{} expression for evaluation via the initial fork, with the master reading results through a pipe. Package \pkg{parallel} supports both this method and the alternative of creating a socket cluster from forked processes.

\item Utilizing operating-system-level facilities to distribute tasks across a group of machines. Multiple technologies exist for this purpose; for instance, package \CRANpkg{snow} can employ MPI (Message Passing Interface) via the \R{} package \CRANpkg{Rmpi}. In such distributed settings, communication overhead can become significant relative to computation time. Consequently, this approach is most effective on tightly-coupled computer networks with high-speed interconnects.

  Other CRAN packages following this paradigm include \CRANpkg{GridR} (using \I{Condor} or \I{Globus}) and \CRANpkg{Rsge} (using \I{SGE}, now known as Oracle Grid Engine).

  This distributed computing approach will not be discussed further in this vignette. However, the components of \pkg{parallel} that provide \CRANpkg{snow}-like functionality are compatible with \CRANpkg{snow} cluster objects, including those configured for MPI.
\end{enumerate}

The hardware landscape for parallel computing has evolved significantly with the proliferation of shared-memory computers featuring multiple CPU cores. Whereas parallel computation was once primarily conducted on clusters of numerous single- or dual-CPU machines, modern laptops commonly possess two to four cores, and servers with 8, 32, or more cores are standard. Package \pkg{parallel} is designed to exploit such hardware effectively. It can also be used across several computers running the same \R{} version and connected via a reasonably fast Ethernet network; these computers need not run the identical operating system.

All communication methods described employ \code{serialize} and \code{unserialize} to transfer \R{} objects between processes. These functions have inherent limits (typically on the order of hundreds of millions of elements), which a well-designed parallel algorithm should not approach.

\section{Numbers of CPUs/cores}

When configuring parallel computations, knowledge of the number of available CPUs or cores can be helpful. However, this is a nuanced concept. Nearly all contemporary physical CPUs contain multiple independent cores (though they may share cache memory and access to RAM). Furthermore, some processor cores support simultaneous multi-threading (hyper-threading), and certain operating systems (e.g., Windows) present \emph{logical} CPUs, whose count may exceed the physical core count.

It is important to note that software can typically only ascertain the total number of CPUs or cores recognized by the operating system. This number may not reflect the resources available to the current user, as system policies on multi-user systems can impose restrictions. Moreover, it provides little guidance on an optimal number of CPUs to allocate for a specific task. The user may be running multiple \R{} processes concurrently, and those processes might themselves use multiple threads via a multi-threaded \abbr{BLAS}, compiled code employing \abbr{OpenMP}, or other low-level parallel techniques. Instances have been observed where \CRANpkg{multicore}'s \code{mclapply} was called recursively,\footnote{The \code{parallel::mclapply} function detects such nested calls and executes them sequentially.} potentially generating $2n + n^2$ processes on a machine with $n = 16$ detected cores.

Despite these complexities, the function \code{detectCores()} attempts to determine the number of CPU cores in the machine where \R{} is executing, using platform-specific methods for all current \R{} platforms. The precise quantity measured is operating-system dependent; the function aims to report the number of \emph{logical} cores available where possible.

On Windows, the default behavior is to report the number of logical CPUs. For modern hardware (e.g., Intel Core i7 processors), this may be reasonable as hyper-threading can offer measurable performance gains. The result of \code{detectCores(logical = FALSE)} depends on the Windows version: recent versions report the number of physical cores, whereas older versions might report the number of physical CPU packages.

\section{Analogues of apply functions}

The most frequent direct application of packages \CRANpkg{multicore} and \CRANpkg{snow} has been to provide parallelized equivalents of the \code{*apply} family of functions, notably \code{lapply}, \code{sapply}, and \code{apply}.

As parallel analogues of \code{lapply}, the package provides:
\begin{verbatim}
parLapply(cl, x, FUN, ...)
mclapply(X, FUN, ..., mc.cores)
\end{verbatim}
Note that \code{mclapply} is unavailable\footnote{Except as a stub function that simply calls \code{lapply}.} on Windows and possesses additional arguments documented on its help page. These functions embody a philosophical difference: \code{mclapply} establishes a transient pool of \code{mc.cores} workers solely for a single computation, whereas \code{parLapply} operates on a more persistent pool defined by a cluster object \code{cl}, created by \code{makeCluster} (which, among other things, specifies the pool size). The typical workflow for \code{parLapply} is:
\begin{verbatim}
cl <- makeCluster(<size of pool>)
# one or more parLapply calls
stopCluster(cl)
\end{verbatim}
% Automatic cleanup upon garbage collection could be arranged.

For matrices, the package includes the less commonly used functions \code{parApply} and \code{parCapply}, as well as the more frequently employed \code{parRapply}, which performs a parallel row-wise apply operation.

\section{SNOW Clusters}

The package incorporates a moderately revised version of substantial portions of \CRANpkg{snow}. Consequently, its functions are also compatible with cluster objects created by \CRANpkg{snow} (provided the \CRANpkg{snow} namespace is accessible).

Two primary functions are supplied for creating SNOW-style clusters: \code{makePSOCKcluster} (a streamlined variant of \code{snow::makeSOCKcluster}) and, on non-Windows platforms, \code{makeForkCluster}. They differ solely in their mechanism for spawning worker processes. \code{makePSOCKcluster} uses \code{Rscript} to launch new \R{} sessions (on the local host or, optionally, remote hosts), while \code{makeForkCluster} creates workers by forking the current \R{} session (thus the workers inherit its environment).

These functions are typically invoked indirectly via the wrapper \code{makeCluster}.

Both the \code{stdout()} and \code{stderr()} connections of worker processes are redirected. By default, output is discarded, but it can be captured in log files using the \code{outfile} argument. It is important to distinguish that this redirection applies to the \R{} connection objects, not the underlying C-level file descriptors. Therefore, output from properly written \R{} packages using \code{Rprintf} will be redirected, whereas direct C-level output may not be.

A default cluster can be designated using \code{setDefaultCluster()}. This cluster will then be used automatically by higher-level functions like \code{parApply} when no explicit cluster argument is provided. Caution is advised when reusing a persistent worker pool, as the workers' workspaces will accumulate objects from previous tasks, and packages may be added to their search paths.

When creating clusters on hosts other than the local machine (\samp{localhost}), additional arguments to \code{makeCluster} may be necessary.
\begin{itemize}
  \item If the worker machines are not identically configured to the master (e.g., different architectures), set \code{homogeneous = FALSE} and potentially specify the full path to \command{Rscript} on the workers via the \code{rscript} argument.
  \item Worker machines must know how to contact the master. Normally, the hostname obtained from \code{Sys.info()} is used, but on private networks this may be incorrect. The \code{master} argument can be supplied as a hostname or IP address (e.g., \code{master = "192.168.1.111"}).
  \item By default, \command{ssh} is used to launch \R{} on remote workers. If the secure shell command has a different name, specify it via \code{rshcmd} (e.g., \code{rshcmd = "plink.exe"} for a Windows system using PuTTY). SSH should be configured for silent (password-less) authentication; setups requiring interactive password entry may fail.
  \item Socket communication occurs over a randomly selected port in the range \code{11000:11999}. If site policies mandate a specific port, use the \code{port} argument or set the environment variable \env{R\_PARALLEL\_PORT}.
\end{itemize}

\section{Forking}

On non-Windows platforms, the package includes functionality derived from \CRANpkg{multicore}. This includes several functions prefixed with \code{mc}, such as \code{mccollect} and \code{mcparallel}. (Package \CRANpkg{multicore} used both prefixed and unprefixed names; the latter were prone to masking—for instance, package \CRANpkg{lattice} formerly contained a function named \code{parallel}.)

The low-level functions from \CRANpkg{multicore} are included but not exported from the \pkg{parallel} namespace.

High-level functions \code{mclapply} and \code{pvec} are provided. Unlike their \CRANpkg{multicore} predecessors, these default to using 2 cores. This default can be controlled by setting \code{options("mc.cores")}, which itself defaults to the value of the environment variable \env{MC\_CORES} when the package is loaded. (Setting this value to \code{1} effectively disables parallel operation; Windows versions are stubs that enforce \code{mc.cores = 1}.)

Functions \code{mcmapply} and \code{mcMap} provide parallel analogues of \code{mapply} and \code{Map}.

Please recall the earlier cautions regarding the use of forking within GUI environments.

The parent and forked child \R{} processes share the per-session temporary directory, \code{tempdir()}. This can be problematic, as considerable existing code assumes this directory is private to a single process. Furthermore, prior to \R{} 2.14.1, it was possible for concurrent calls to \code{tempfile} in two processes to generate the same filename, potentially leading to conflicts.

Forked workers share open file handles with the master process. Consequently, output directed to \file{stdout} and \file{stderr} from a worker typically appears alongside the master's output. (This behavior is not entirely reliable on all operating systems; issues have also been noted when forking a session that is processing batch input from \file{stdin}.) Setting the argument \code{mc.silent = TRUE} suppresses \file{stdout} output from child processes; \file{stderr} is unaffected.

The sharing of file handles also impacts graphics. Forked workers inherit all open graphics devices from the parent and should not attempt to use them.

\section{Random-number generation}

Parallel computations involving (pseudo-)random numbers require careful consideration. The separate processes or threads executing distinct portions of the computation must utilize independent—and preferably reproducible—random-number streams. One straightforward strategy is to perform all randomization within the master process before distributing tasks; this approach is used where feasible in \CRANpkg{boot} (version 1.3-1 and later).

When an \R{} process starts, it initializes its random-number seed from the \code{.Random.seed} object in a saved workspace or, if random-number generation is invoked for the first time, constructs a seed based on the current time and process ID (see \code{?RNG}). Thus, worker processes might receive identical seeds if a workspace containing \code{.Random.seed} was restored, or if the random-number generator was used prior to forking. Otherwise, each worker obtains a non-reproducible seed (though with very high probability, a unique seed per worker).

An alternative approach is to assign each worker a distinct seed derived reproducibly from the master's seed. This is generally sufficiently safe, though concerns have been raised about the possibility of random-number streams in different workers becoming correlated. One method is to space the seeds far apart within a single random-number stream; however, numbers separated by a large fixed distance in a stream are not necessarily more independent than those closer together. Another technique, employed by software such as JAGS, is to use entirely different random-number generator algorithms for each parallel run or process.

Package \pkg{parallel} includes an implementation of the scheme proposed by \citet{lecuyer.2002}. This method uses a single underlying random-number generator to create multiple \emph{streams}, with seeds spaced $2^{127}$ steps apart within a very long-period sequence (approximately $2^{191}$). The implementation is based on the generator described by \citet{lecuyer.1999}. The rationale for selecting this generator\footnote{Beyond the common authorship!} is its long period achievable with a compact seed (six integers) and, unlike \R{}'s default Mersenne-Twister, the straightforward ability to advance its state by a predetermined number of steps. The generator combines two recursive modulo operations:
\begin{eqnarray*}
  x_n &=& 1403580 \times x_{n-2} - 810728 \times x_{n-3} \mod{(2^{32} - 209)}\\
  y_n &=& 527612 \times y_{n-1} - 1370589 \times y_{n-3} \mod{(2^{32} - 22853)}\\
  z_n &=& (x_n - y_n) \mod{4294967087}\\
  u_n &=& z_n/4294967088\ \mbox{unless $z_n = 0$}
\end{eqnarray*}
The \emph{seed} is the six-integer vector $(x_{n-3}, x_{n-2}, x_{n-1}, y_{n-3}, y_{n-2}, y_{n-1})$. Pre-computed coefficients allow advancing each of the $x$ and $y$ sequences by $k$ steps efficiently. For $k = 2^{127}$, the seed is advanced by calling \code{.Random.seed <- nextRNGStream(.Random.seed)} in \R.

% Use \verb for consistent quote handling.
The \citet{lecuyer.1999} generator is accessible in \R{} via \code{RNGkind("L'Ecuyer-CMRG")}. Employing the \citet{lecuyer.2002} scheme for parallel streams is therefore straightforward:
<<Ecuyer-ex, eval=FALSE>>=
RNGkind("L'Ecuyer-CMRG")
set.seed(2002) # choose an initial seed
M <- 16 ## number of workers
s <- .Random.seed
for (i in 1:M) {
    s <- nextRNGStream(s)
    # send s to worker i as its .Random.seed
}
@
This mechanism is integrated into \code{clusterSetRNGStream} for SNOW clusters and is the default behavior for \code{mcparallel} and \code{mclapply}.

In addition to \emph{streams} ($2^{127}$ steps apart), the framework defines \emph{sub-streams}, initiated from seeds $2^{76}$ steps apart. Function \code{nextRNGSubStream} advances to the next sub-stream.

A direct \R{} interface to the original (more cumbersome) C implementation is available in the CRAN package \CRANpkg{rlecuyer} \citep{rlecuyer}, which operates with named streams, each associated with three six-element seeds. This can be emulated in base \R{} by storing and managing \code{.Random.seed} values appropriately. Another S4 class-based interface is provided by package \CRANpkg{rstream} \citep{rstream}.

\section{Load balancing}

The introduction mentioned an alternative strategy that dynamically allocates tasks to workers as they become available, often termed \emph{load balancing}. This is implemented in \code{mclapply(mc.preschedule = FALSE)}, \code{clusterApplyLB}, and related wrapper functions.

Load balancing can be advantageous when individual tasks exhibit substantial variation in computation time, or when the participating nodes have heterogeneous processing capabilities. However, certain caveats apply:
\begin{enumerate}[(a)]
\item Random-number streams are allocated to nodes, not to individual tasks. Therefore, if tasks involve random-number generation, the overall computation may not be repeatable, as the assignment of tasks to specific nodes depends on relative node completion times. Modifying the mechanism to allocate a stream per task would require only minor adjustments.
\item Task granularity requires consideration. If 1000 tasks are to be distributed among 10 nodes, the standard (prescheduled) approach sends blocks of 100 tasks to each node. The load-balancing approach sends tasks individually, which can incur significant communication overhead if the number of tasks is excessively large relative to the number of nodes. Thus, it is sensible to have notably more tasks than nodes, but a ratio of 100:1 (or even 10:1) may be inefficient.
\end{enumerate}

\section{Setting the CPU Affinity with \code{mclapply}}
%%\author{Helena Kotthaus, Andreas Lang \\ LS12 Department of Computer Science, TU Dortmund}

The \code{affinity.list} parameter of the \code{mclapply} function allows the user to specify which CPU cores (or hyper-thread units) are permitted to execute each element of the input vector \code{X}. The \code{affinity.list} is a vector (atomic or list) containing a CPU affinity mask for each corresponding element of \code{X}, dictating the allowable CPUs for that task (see \code{? mcaffinity}). This capability can be beneficial when parallel jobs exhibit high variance in completion time or when executing on heterogeneous hardware architectures. It also enables the exploration of scheduling strategies to optimize the overall runtime of independent parallel jobs. When \code{affinity.list} is specified, the \code{mc.cores} parameter is effectively overridden by the count of unique CPU IDs present in the affinity masks. Using this parameter requires deactivating prescheduling (\code{mc.preschedule = FALSE}). A separate child process is forked for each element of \code{X}. The master process forks only one child per selected CPU at any moment, ensuring the number of concurrent processes does not exceed the number of designated CPUs. Once a child process completes, the next available job assigned to that CPU is initiated.

The following code example illustrates how execution time can be reduced by strategically assigning elements of \code{X} to specific CPUs. Note that this example is not applicable on Windows, where \code{mc.cores} is restricted to 1.

%%     setting eval=TRUE  would cost ca. 1.5 minutes every time this is built
%%     also,   .Platform$OS.type == "Windows"  cannot use mc.cores = 2
<<affinity-ex, eval=FALSE>>=
## Exemplary variance filter executed on three different matrices in parallel.
## Can be used in gene expression analysis as a prefilter
## for the number of covariates.

library(parallel)
n <- 300   # observations
p <- 20000 # covariates

## Different sized matrices as filter inputs
## Matrix A and B form smaller work loads
## while matrix C forms a bigger workload (2*p)
library(stats)
A <- matrix(replicate( p,  rnorm(n, sd = runif(1, 0.1, 10))), n, p)
B <- matrix(replicate( p,  rnorm(n, sd = runif(1, 0.1, 10))), n, p)
C <- matrix(replicate(2*p, rnorm(n, sd = runif(1, 0.1, 10))), n, 2*p)

varFilter <- function (X, nSim = 20) {
  for (i in 1:nSim) {
    train <- sample(nrow(X), 2 / 3 * nrow(X))
    colVars <- apply(X[train, ], 2, var)
    keep <- names(head(sort(colVars, decreasing = TRUE), 100))
    # myAlgorithm(X[, keep])
  }
}

## Runtime comparison -----------------------------------

## mclapply with affinity.list
## CPU mapping: A and B run on CPU 1 while C runs on CPU 2:
affinity <- c(1,1,2)
system.time(
  mclapply(X = list(A,B,C), FUN = varFilter,
           mc.preschedule = FALSE, affinity.list = affinity))
##   user  system elapsed
## 34.909   0.873  36.720


## mclapply without affinity.list
system.time(
  mclapply(X = list(A,B,C), FUN = varFilter, mc.cores = 2,
           mc.preschedule = FALSE) )
##   user  system elapsed
## 72.893   1.588  55.982


## mclapply with prescheduling
system.time(
   mclapply(X = list(A,B,C), FUN = varFilter, mc.cores = 2,
            mc.preschedule = TRUE) )
##   user  system elapsed
## 53.455   1.326  53.399
@

Beyond runtime optimization, \code{affinity.list} can also be used simply to restrict computation to a specific subset of CPUs, as shown in the following trivial example.

<<eval = FALSE>>=
## Restricts all elements of X to run on CPU 1 and 2.
X <- list(1, 2, 3)
affinity.list <- list(c(1,2), c(1,2), c(1,2))
mclapply(X = X, FUN = function (i) i*i,
         mc.preschedule = FALSE, affinity.list = affinity.list)
@


\section{Portability considerations}

Developers intending to incorporate parallel capabilities into their code must balance portability and efficiency, as no single approach works optimally across all platforms.

Using \code{mclapply} is often the simplest method but results in serial execution on Windows. This may be acceptable if parallel computation is only needed on a multi-core Unix-alike server, since \code{mclapply} operates only within a single shared-memory system. Serial fallback is available by setting \code{mc.cores = 1}.

Using \code{parLapply} works wherever socket communication is permitted and can harness cores across multiple machines, such as an idle computer lab. However, socket communication may be blocked even on a single machine and is frequently restricted between machines on institutional networks. There is no built-in automatic fallback to serial execution, nor would one be trivial to implement, as workers begin with a different \R{} environment than the master.

An example of providing access to both parallel approaches alongside a serial option is package \CRANpkg{boot}, version \code{1.3-3} and later.

\section{Extended examples}

\SweaveOpts{eval=FALSE}
<<hide=TRUE>>=
library(parallel)
@

One of the most prevalent applications of coarse-grained parallelization in statistics is the execution of multiple simulation runs, such as computing large numbers of bootstrap replicates or performing several independent MCMC chains. We present an example of each type.

Note that some examples will execute only serially on Windows, and several are computationally intensive.

\subsection{Bootstrapping}

Package \CRANpkg{boot} \citep{boot} provides supporting software for the monograph by \citet{Davison.Hinkley.97}. Bootstrapping is frequently cited as an archetypal task for easy parallelization, as certain methods for constructing confidence intervals require thousands of bootstrap samples. Starting with version \code{1.3-1}, the \CRANpkg{boot} package incorporates internal parallel support within its main functions. Nevertheless, we illustrate how to utilize the original (serial) functions within a parallel framework.

We examine two examples using the \code{cd4} dataset from \CRANpkg{boot}, where the focus is the correlation between pre- and post-treatment measurements. The first example is a straightforward parametric bootstrap simulation. The serial implementation is:
<<>>=
library(boot)
cd4.rg <- function(data, mle) MASS::mvrnorm(nrow(data), mle$m, mle$v)
cd4.mle <- list(m = colMeans(cd4), v = var(cd4))
cd4.boot <- boot(cd4, corr, R = 999, sim = "parametric",
                 ran.gen = cd4.rg, mle = cd4.mle)
boot.ci(cd4.boot,  type = c("norm", "basic", "perc"),
        conf = 0.9, h = atanh, hinv = tanh)
@

To parallelize this using \code{mclapply}, we divide the replicates into separate runs. Here we illustrate two runs of 500 simulations each:
<<>>=
cd4.rg <- function(data, mle) MASS::mvrnorm(nrow(data), mle$m, mle$v)
cd4.mle <- list(m = colMeans(cd4), v = var(cd4))
run1 <- function(...) boot(cd4, corr, R = 500, sim = "parametric",
                           ran.gen = cd4.rg, mle = cd4.mle)
mc <- 2 # set as appropriate for your hardware
## To make this reproducible:
set.seed(123, "L'Ecuyer")
cd4.boot <- do.call(c, mclapply(seq_len(mc), run1) )
boot.ci(cd4.boot,  type = c("norm", "basic", "perc"),
        conf = 0.9, h = atanh, hinv = tanh)
@
Numerous programming patterns exist for this task; a clean approach often involves encapsulating the computation within a function. The parallel code above is structurally similar to the serial equivalent:
<<eval=FALSE>>=
do.call(c, lapply(seq_len(mc), run1))
@

To execute this using \code{parLapply}, a comparable strategy can be employed:
<<>>=
run1 <- function(...) {
   library(boot)
   cd4.rg <- function(data, mle) MASS::mvrnorm(nrow(data), mle$m, mle$v)
   cd4.mle <- list(m = colMeans(cd4), v = var(cd4))
   boot(cd4, corr, R = 500, sim = "parametric",
        ran.gen = cd4.rg, mle = cd4.mle)
}
cl <- makeCluster(mc)
## make this reproducible
clusterSetRNGStream(cl, 123)
library(boot) # needed for c() method on master
cd4.boot <- do.call(c, parLapply(cl, seq_len(mc), run1) )
boot.ci(cd4.boot,  type = c("norm", "basic", "perc"),
        conf = 0.9, h = atanh, hinv = tanh)
stopCluster(cl)
@
A key distinction is that \code{mclapply} automatically makes packages and objects from the master's environment available to forked workers, whereas this is not generally\footnote{It is the case for clusters created with \code{makeForkCluster}.} true for \code{parLapply}. Decisions about where to perform computations require consideration; for instance, we could compute \code{cd4.mle} on each worker (as above) or compute it once on the master and transmit the value. The latter approach is illustrated below:
<<>>=
cl <- makeCluster(mc)
cd4.rg <- function(data, mle) MASS::mvrnorm(nrow(data), mle$m, mle$v)
cd4.mle <- list(m = colMeans(cd4), v = var(cd4))
clusterExport(cl, c("cd4.rg", "cd4.mle"))
junk <- clusterEvalQ(cl, library(boot)) # discard result
clusterSetRNGStream(cl, 123)
res <- clusterEvalQ(cl, boot(cd4, corr, R = 500,
                    sim = "parametric", ran.gen = cd4.rg, mle = cd4.mle))
library(boot) # needed for c() method on master
cd4.boot <- do.call(c, res)
boot.ci(cd4.boot,  type = c("norm", "basic", "perc"),
        conf = 0.9, h = atanh, hinv = tanh)
stopCluster(cl)
@

Executing a double bootstrap for the same problem is substantially more demanding computationally. The standard serial implementation is:
<<fig=TRUE>>=
R <- 999; M <- 999 ## we would like at least 999 each
cd4.nest <- boot(cd4, nested.corr, R=R, stype="w", t0=corr(cd4), M=M)
## nested.corr is a function in package boot
op <- par(pty = "s", xaxs = "i", yaxs = "i")
qqplot((1:R)/(R+1), cd4.nest$t[, 2], pch = ".", asp = 1,
        xlab = "nominal", ylab = "estimated")
abline(a = 0, b = 1, col = "grey")
abline(h = 0.05, col = "grey")
abline(h = 0.95, col = "grey")
par(op)

nominal <- (1:R)/(R+1)
actual <- cd4.nest$t[, 2]
100*nominal[c(sum(actual <= 0.05), sum(actual < 0.95))]
@
This required approximately 55 seconds on one core of an 8-core Linux server.

A parallel version using \code{mclapply} could be structured as:
<<eval=FALSE>>=
mc <- 9
R <- 999; M <- 999; RR <- floor(R/mc)
run2 <- function(...)
    cd4.nest <- boot(cd4, nested.corr, R=RR, stype="w", t0=corr(cd4), M=M)
cd4.nest <- do.call(c, mclapply(seq_len(mc), run2, mc.cores = mc) )
nominal <- (1:R)/(R+1)
actual <- cd4.nest$t[, 2]
100*nominal[c(sum(actual <= 0.05), sum(actual < 0.95))]
@
This parallel execution completed in 11 seconds (elapsed time) utilizing all cores of the server.

\subsection{MCMC runs}

\citet{Ripley.88} discusses maximum-likelihood estimation for the Strauss spatial point process, which involves solving a moment equation
\[
E_c T = t
\]
where $T$ denotes the number of $R$-close pairs and $t$ is the observed value (30 in the following example). A serial exploratory analysis might proceed as:
<<>>=
library(spatial)
towns <- ppinit("towns.dat")
tget <- function(x, r=3.5) sum(dist(cbind(x$x, x$y)) < r)
t0 <- tget(towns)
R <- 1000
c <- seq(0, 1, 0.1)
## res[1] = 0
res <- c(0, sapply(c[-1], function(c)
    mean(replicate(R, tget(Strauss(69, c=c, r=3.5))))))
plot(c, res, type="l", ylab="E t")
abline(h=t0, col="grey")
@
This computation takes about 20 seconds on contemporary hardware but required many hours when originally performed in 1985. A parallel version could be implemented as:
<<>>=
run3 <- function(c) {
    library(spatial)
    towns <- ppinit("towns.dat") # has side effects
    mean(replicate(R, tget(Strauss(69, c=c, r=3.5))))
}
cl <- makeCluster(10, methods = FALSE)
clusterExport(cl, c("R", "towns", "tget"))
res <- c(0, parSapply(cl, c[-1], run3)) # 10 tasks
stopCluster(cl)
@
This required approximately 4.5 seconds of computation plus 2 seconds for cluster setup. Employing a fork cluster (not on Windows) significantly reduces startup time and simplifies setup:
<<eval=FALSE>>=
cl <- makeForkCluster(10)  # fork after the variables have been set up
run4 <- function(c)  mean(replicate(R, tget(Strauss(69, c=c, r=3.5))))
res <- c(0, parSapply(cl, c[-1], run4))
stopCluster(cl)
@
As expected, the \code{mclapply} version is somewhat more concise:
<<eval=FALSE>>=
run4 <- function(c)  mean(replicate(R, tget(Strauss(69, c=c, r=3.5))))
res <- c(0, unlist(mclapply(c[-1], run4, mc.cores = 10)))
@
If fewer than 10 cores are available, load balancing may be worth considering for this task, as simulation time varies with the parameter \code{c}. This can be achieved using \code{mclapply(mc.preschedule = FALSE)} or \code{parSapplyLB}. A trade-off is that results become non-reproducible (which may be acceptable in this context).

\subsection{Package installation}

With thousands of \R{} packages available, performing large-scale parallel installations can be efficient. The \code{install.packages} function offers a parallel mode via \command{make}, but this approach is not universally suitable.\footnote{A parallel \command{make} may be unavailable, and occasional issues have been observed with package installation when run under \command{make}.} Installation times for packages can vary dramatically, and these durations are not known in advance.

We illustrate an alternative approach using the \pkg{parallel} package, as employed on parts of the CRAN check farm. Suppose a function \code{do\_one(pkg)} exists that installs a single package and returns. The objective is to run \code{do\_one} on as many of $M$ workers as possible while respecting the dependency graph: a package must not be installed until all its direct and indirect dependencies have been installed. Because installing one package can block others, the number of concurrent installations must be dynamic. The following code achieves this using low-level parallel functions.

<<eval=FALSE>>=
pkgs <- "<names of packages to be installed>"
M <- 20 # number of parallel installs
M <- min(M, length(pkgs))
library(parallel)
unlink("install_log")
cl <- makeCluster(M, outfile = "install_log")
clusterExport(cl, c("tars", "fakes", "gcc")) # variables needed by do_one

## set up available via a call to available.packages() for
## repositories containing all the packages involved and all their
## dependencies.
DL <- utils:::.make_dependency_list(pkgs, available, recursive = TRUE)
DL <- lapply(DL, function(x) x[x %in% pkgs])
lens <- sapply(DL, length)
ready <- names(DL[lens == 0L])
done <- character() # packages already installed
n <- length(ready)
submit <- function(node, pkg)
    parallel:::sendCall(cl[[node]], do_one, list(pkg), tag = pkg)
for (i in 1:min(n, M)) submit(i, ready[i])
DL <- DL[!names(DL) %in% ready[1:min(n, M)]]
av <- if(n < M) (n+1L):M else integer() # available workers
while(length(done) < length(pkgs)) {
    d <- parallel:::recvOneResult(cl)
    av <- c(av, d$node)
    done <- c(done, d$tag)
    OK <- unlist(lapply(DL, function(x) all(x %in% done) ))
    if (!any(OK)) next
    p <- names(DL)[OK]
    m <- min(length(p), length(av)) # >= 1
    for (i in 1:m) submit(av[i], p[i])
    av <- av[-(1:m)]
    DL <- DL[!names(DL) %in% p[1:m]]
}
@

\subsection{Passing \code{...}}

The semantics of the special argument \code{...} (dot-dot-dot) are not naturally aligned with parallel execution because lazy evaluation of promises may be deferred until after tasks have been dispatched to workers. This poses no difficulty in the forking model, as forked workers possess the complete evaluation environment.

For \CRANpkg{snow}-like clusters, the key is to force evaluation of any promises within \code{...} while the necessary context is still available on the master. This is the technique used by \CRANpkg{boot}:
<<eval=FALSE>>=
    fn <- function(r) statistic(data, i[r, ], ...)
    RR <- sum(R)
    res <- if (ncpus > 1L && (have_mc || have_snow)) {
        if (have_mc) {
            parallel::mclapply(seq_len(RR), fn, mc.cores = ncpus)
        } else if (have_snow) {
            list(...) # evaluate any promises
            if (is.null(cl)) {
                cl <- parallel::makePSOCKcluster(rep("localhost", ncpus))
                if(RNGkind()[1L] == "L'Ecuyer-CMRG")
                    parallel::clusterSetRNGStream(cl)
                res <- parallel::parLapply(cl, seq_len(RR), fn)
                parallel::stopCluster(cl)
                res
            } else parallel::parLapply(cl, seq_len(RR), fn)
        }
    } else lapply(seq_len(RR), fn)
@
Note that \code{...} is an argument to \code{boot}. The line
<<eval=FALSE>>=
            list(...) # evaluate any promises
@
forces evaluation, causing the promises to resolve within the evaluation frame of \code{boot}. Consequently, the objects become part of the environment of \code{fn} and are transmitted to the workers along with the function definition.

\section{Differences from earlier versions}

The implementation of parallel random-number generation differs from that in \CRANpkg{snow}; the original \CRANpkg{multicore} package provided no such support.

\subsection{Differences from multicore}

\CRANpkg{multicore} contained elaborate code to suppress the Aqua event loop in \command{R.app} and the event loop for the \code{quartz} graphics device. This has been replaced by setting an internal flag in the \R{} executable upon forking.

Functions \code{fork} and \code{kill} are prefixed with \code{mc} (\code{mcfork}, \code{mckill}) and are not exported from the namespace. This avoids naming conflicts with other packages (e.g., \CRANpkg{fork}) and clarifies that \code{mckill} is less general than \code{tools::pskill}.

Aliased functions \code{collect} and \code{parallel} (without the \code{mc} prefix) are no longer provided.

\subsection{Differences from snow}

\CRANpkg{snow} set a socket timeout exceeding the maximum value stipulated by POSIX and did not allow configuration of the timeout on worker processes. This could lead to process deadlocks on Solaris.

In \pkg{parallel}, \code{makeCluster} creates MPI %% or NWS
clusters by calling the corresponding functions from \CRANpkg{snow} if that package is available.

\code{makePSOCKcluster} has been simplified, as package \pkg{parallel} resides in a known location and \command{Rscript} is now universally available. Worker process logging is configured to append to the specified file, allowing multiple processes to share a log.

\code{parSapply} has been adjusted to align its behavior more closely with standard \code{sapply}.

\code{clusterMap()} has been extended with \code{SIMPLIFY} and \code{USE.NAMES} arguments, making it a full parallel counterpart to \code{mapply} and \code{Map}.

The timing interface present in \CRANpkg{snow} has not been duplicated.

\bibliographystyle{jss}
\bibliography{parallel}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: